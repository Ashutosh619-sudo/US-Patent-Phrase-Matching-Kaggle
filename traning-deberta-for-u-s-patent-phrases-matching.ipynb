{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T17:17:00.280670Z","iopub.execute_input":"2022-05-18T17:17:00.280945Z","iopub.status.idle":"2022-05-18T17:17:00.305526Z","shell.execute_reply.started":"2022-05-18T17:17:00.280915Z","shell.execute_reply":"2022-05-18T17:17:00.304701Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:00.495169Z","iopub.execute_input":"2022-05-18T17:17:00.495468Z","iopub.status.idle":"2022-05-18T17:17:00.573287Z","shell.execute_reply.started":"2022-05-18T17:17:00.495434Z","shell.execute_reply":"2022-05-18T17:17:00.572424Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:00.706494Z","iopub.execute_input":"2022-05-18T17:17:00.707244Z","iopub.status.idle":"2022-05-18T17:17:01.255011Z","shell.execute_reply.started":"2022-05-18T17:17:00.707199Z","shell.execute_reply":"2022-05-18T17:17:01.253928Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cpc = cpc.rename(columns = {\"code\" : \"context\"})\ntrain = pd.merge(train, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.257461Z","iopub.execute_input":"2022-05-18T17:17:01.257835Z","iopub.status.idle":"2022-05-18T17:17:01.417654Z","shell.execute_reply.started":"2022-05-18T17:17:01.257788Z","shell.execute_reply":"2022-05-18T17:17:01.416568Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train['sen1'] = train['anchor'].astype('str')+' '+train['title'].astype('str')\ntrain = train.drop(['anchor','context','title'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.419458Z","iopub.execute_input":"2022-05-18T17:17:01.419802Z","iopub.status.idle":"2022-05-18T17:17:01.446470Z","shell.execute_reply.started":"2022-05-18T17:17:01.419756Z","shell.execute_reply":"2022-05-18T17:17:01.445655Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.448734Z","iopub.execute_input":"2022-05-18T17:17:01.449066Z","iopub.status.idle":"2022-05-18T17:17:01.464252Z","shell.execute_reply.started":"2022-05-18T17:17:01.449010Z","shell.execute_reply":"2022-05-18T17:17:01.463086Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(train[['target','sen1']],train['score'],random_state=1234,test_size=0.3)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.465746Z","iopub.execute_input":"2022-05-18T17:17:01.466575Z","iopub.status.idle":"2022-05-18T17:17:01.486224Z","shell.execute_reply.started":"2022-05-18T17:17:01.466517Z","shell.execute_reply":"2022-05-18T17:17:01.485403Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom torch.nn.utils.clip_grad import clip_grad_norm\nimport torch_xla\nimport torch_xla.core.xla_model as xm","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.488385Z","iopub.execute_input":"2022-05-18T17:17:01.488710Z","iopub.status.idle":"2022-05-18T17:17:01.519165Z","shell.execute_reply.started":"2022-05-18T17:17:01.488668Z","shell.execute_reply":"2022-05-18T17:17:01.515657Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.549781Z","iopub.execute_input":"2022-05-18T17:17:01.549992Z","iopub.status.idle":"2022-05-18T17:17:01.619137Z","shell.execute_reply.started":"2022-05-18T17:17:01.549966Z","shell.execute_reply":"2022-05-18T17:17:01.618367Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class PatentModel(nn.Module):\n    def __init__(self,bert_path):\n        super(PatentModel,self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.AutoModel.from_pretrained(self.bert_path)\n        self.fc_layer = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(768,1),\n            nn.Sigmoid()\n        )\n    def forward(self,ids,mask,token_type_ids):\n        out = self.bert(input_ids=ids,attention_mask=mask,token_type_ids=token_type_ids)\n        last_hidden_state = out.last_hidden_state\n        cls_embeddings = last_hidden_state[:, 0]\n        bo = self.fc_layer(cls_embeddings)\n        return bo","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:01.733642Z","iopub.execute_input":"2022-05-18T17:17:01.734341Z","iopub.status.idle":"2022-05-18T17:17:01.742544Z","shell.execute_reply.started":"2022-05-18T17:17:01.734291Z","shell.execute_reply":"2022-05-18T17:17:01.741393Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class PatentDataset:\n    def __init__(self,text1,text2,label,tokenizer,max_len):\n        self.text1=text1\n        self.text2=text2\n        self.label=label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        label = self.label[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\": torch.tensor(label,dtype=torch.float),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:06.222343Z","iopub.execute_input":"2022-05-18T17:17:06.223125Z","iopub.status.idle":"2022-05-18T17:17:06.235057Z","shell.execute_reply.started":"2022-05-18T17:17:06.223082Z","shell.execute_reply":"2022-05-18T17:17:06.233558Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"max_len=128\ntrain_batch_size = 8\nepochs=4\ndeberta_path = '../input/huggingface-deberta-variants/deberta-base/deberta-base'\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(deberta_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:06.411881Z","iopub.execute_input":"2022-05-18T17:17:06.412604Z","iopub.status.idle":"2022-05-18T17:17:06.887302Z","shell.execute_reply.started":"2022-05-18T17:17:06.412563Z","shell.execute_reply":"2022-05-18T17:17:06.886431Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_text1 = list(x_train['target'].values)\ntrain_text2 = list(x_train['sen1'].values)\ntrain_label = list(y_train.values)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.521342Z","iopub.execute_input":"2022-05-18T17:17:08.522065Z","iopub.status.idle":"2022-05-18T17:17:08.533318Z","shell.execute_reply.started":"2022-05-18T17:17:08.522013Z","shell.execute_reply":"2022-05-18T17:17:08.532410Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataset = PatentDataset(text1=train_text1,text2 = train_text2,label=train_label,tokenizer = tokenizer,max_len=max_len)\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset,batch_size=train_batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.535396Z","iopub.execute_input":"2022-05-18T17:17:08.535963Z","iopub.status.idle":"2022-05-18T17:17:08.544654Z","shell.execute_reply.started":"2022-05-18T17:17:08.535915Z","shell.execute_reply":"2022-05-18T17:17:08.543803Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"val_text1 = list(x_test['target'].values)\nval_text2 = list(x_test['sen1'].values)\nval_label = list(y_test.values)\n\nvalid_dataset = PatentDataset(text1=val_text1,text2 = val_text2,label=val_label,tokenizer = tokenizer,max_len=max_len)\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=train_batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.547235Z","iopub.execute_input":"2022-05-18T17:17:08.547764Z","iopub.status.idle":"2022-05-18T17:17:08.557568Z","shell.execute_reply.started":"2022-05-18T17:17:08.547722Z","shell.execute_reply":"2022-05-18T17:17:08.556782Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer,scheduler,loss_fun,epochs,train_loader,val_loader,device,clip_val=2):\n        \n    model.train()\n    for epoch in range(epochs):\n        losses = []\n        for step,batch in enumerate(train_loader):\n            batch_inputs, batch_masks, batch_labels = batch[\"ids\"].to(device), batch[\"mask\"].to(device), batch[\"targets\"].to(device)\n            batch_token_type_ids = batch[\"token_type_ids\"]\n            model.zero_grad()\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n            loss = loss_fun(outputs.squeeze(),batch_labels.squeeze())\n            losses.append(loss)\n            loss.backward()\n            clip_grad_norm(model.parameters(),clip_val)\n            optimizer.step()\n            scheduler.step()\n        loss2 = sum(best_loss)/len(best_loss)\n        print(f'Epoch : {epoch} ,Train loss : {loss2}')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.559066Z","iopub.execute_input":"2022-05-18T17:17:08.559620Z","iopub.status.idle":"2022-05-18T17:17:08.570105Z","shell.execute_reply.started":"2022-05-18T17:17:08.559582Z","shell.execute_reply":"2022-05-18T17:17:08.569262Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def r2_score(outputs, labels):\n    labels_mean = torch.mean(labels)\n    ss_tot = torch.sum((labels - labels_mean) ** 2)\n    ss_res = torch.sum((labels - outputs) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return r2","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.572225Z","iopub.execute_input":"2022-05-18T17:17:08.572792Z","iopub.status.idle":"2022-05-18T17:17:08.580673Z","shell.execute_reply.started":"2022-05-18T17:17:08.572749Z","shell.execute_reply":"2022-05-18T17:17:08.579917Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def evaluate(model,loss_function,test_dataloader,device):\n    model.eval()\n    test_loss, test_r2 = [], []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        loss = loss_function(outputs, batch_labels)\n        test_loss.append(loss.item())\n        r2 = r2_score(outputs, batch_labels)\n        test_r2.append(r2.item())\n    return test_loss, test_r2","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.582610Z","iopub.execute_input":"2022-05-18T17:17:08.583157Z","iopub.status.idle":"2022-05-18T17:17:08.591789Z","shell.execute_reply.started":"2022-05-18T17:17:08.583116Z","shell.execute_reply":"2022-05-18T17:17:08.591078Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"num_train_steps = len(train_data_loader) * epochs\nmodel = PatentModel(deberta_path).to(device)\n\noptimizer = transformers.AdamW(model.parameters(),lr=3e-5,eps=1e-8)\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_train_steps\n)\nloss_function = nn.MSELoss()\n\nmodel = train(model, optimizer, scheduler, loss_function, epochs,train_data_loader, valid_data_loader,device)\nloss1,r2_ = evaluate(model,loss_function,valid_data_loader,device)\n\nloss = sum(loss1)/len(loss1)\nr2 = sum(r2_)/len(r2_)\nprint(f\"eval mean result : loss {loss}, r2 {r2}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T17:17:08.592934Z","iopub.execute_input":"2022-05-18T17:17:08.595268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}